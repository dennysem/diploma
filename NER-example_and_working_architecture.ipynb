{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D\n",
    "from keras import regularizers\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.37 s, sys: 2.15 s, total: 8.52 s\n",
      "Wall time: 8.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_json(\"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['query']\n",
    "y = data['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'query':X, 'tags': y})\n",
    "#data = shuffle(data, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.head(15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_json(\"data1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['query']\n",
    "y = data['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: query, dtype: object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padd_data(data, max_len=29, value=0, mode='post'):\n",
    "    return pad_sequences(data, maxlen=max_len, value=value, padding=mode, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 232 ms, sys: 14.2 ms, total: 246 ms\n",
      "Wall time: 242 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = padd_data(X_train, max_len=29, value=0)\n",
    "X_test = padd_data(X_test, max_len=29, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 255 ms, sys: 31.1 ms, total: 286 ms\n",
      "Wall time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train = padd_data(y_train, max_len=29, value=0)\n",
    "y_test = padd_data(y_test, max_len=29, value=0)\n",
    "y_train[y_train == '0.0'] = 'stop_word'\n",
    "y_test[y_test == '0.0'] = 'stop_word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-filter', 'B-predefined_filter', 'B-filter_value', 'B-dimension', 'stop_word', 'I-predefined_filter', 'I-filter', 'B-metric', 'I-dimension', 'I-metric']\n"
     ]
    }
   ],
   "source": [
    "all_tags = list(set(y_train.reshape((y_train.shape[0] * y_train.shape[1]))))\n",
    "print (all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 184 ms, sys: 1.37 ms, total: 185 ms\n",
      "Wall time: 185 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = list(set(X_train.reshape((X_train.shape[0] * X_train.shape[1]))))\n",
    "b = list(set(X_test.reshape((X_test.shape[0] * X_test.shape[1]))))\n",
    "a = list(set(a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2numb = dict(zip(sorted(a), list(range(len(a)))))\n",
    "numb2word = dict(zip(list(range(len(a))), sorted(a)))\n",
    "tag2numb = dict(zip(sorted(all_tags), list(range(len(all_tags)))))\n",
    "numb2tag = dict(zip(list(range(len(all_tags))), sorted(all_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "310\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(all_tags)\n",
    "n_vocab = len(word2numb)\n",
    "print(n_classes)\n",
    "print(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 652 ms, sys: 6.97 ms, total: 659 ms\n",
      "Wall time: 659 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_numb = np.array([list(map(lambda x: word2numb[x], w)) for w in X_train])\n",
    "X_test_numb = np.array([list(map(lambda x: word2numb[x], w)) for w in X_test])\n",
    "y_train_numb = np.array([list(map(lambda x: tag2numb[x], y)) for y in y_train])\n",
    "y_test_numb = np.array([list(map(lambda x: tag2numb[x], y)) for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    #print (y_true)\n",
    "    #print (y_pred)\n",
    "    assert y_true.shape == y_pred.shape \n",
    "    return sum(list(int(np.all(y_true[i] == y_pred[i])) for i in range(y_pred.shape[0]))) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(model, x=X_test_numb):\n",
    "    labels_pred = []\n",
    "\n",
    "    for n_batch, sent in tqdm(enumerate(x)):\n",
    "        pred = model.predict_on_batch(sent[np.newaxis,:])\n",
    "        labels_pred.append(np.argmax(pred,-1)[0])\n",
    "\n",
    "    labels_pred = [ list(map(lambda x: numb2tag[x], y)) for y in labels_pred]\n",
    "    return labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(n_epochs = 2):\n",
    "    for i in tqdm(range(n_epochs)):\n",
    "        print(\"Epoch {}\".format(i))\n",
    "\n",
    "        for n_batch, sent in tqdm(enumerate(X_train_numb)):\n",
    "            # Make labels one hot\n",
    "            label = np.eye(len(all_tags))[y_train_numb[n_batch]][np.newaxis,:] \n",
    "            # View each sentence as a batch\n",
    "            sent = sent[np.newaxis,:]\n",
    "            if sent.shape[1] > 1: #ignore 1 word sentences\n",
    "                model.train_on_batch(sent, label)\n",
    "        print (\"Train accuracy: \", get_accuracy(np.array(prediction(model, X_train_numb)), np.array(y_train)))\n",
    "        print (\"Validation accuracy: \", get_accuracy(np.array(prediction(model, X_test_numb)), np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, 100))\n",
    "model.add(Convolution1D(128, 4, padding='same', activation='relu'))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.compile('rmsprop', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b5e21ba78f4742bbd79cd86120d953"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5118739f274472a546aaa9acea2c2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101a165b0e28436ba1b67be0d45ac88e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9804666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9bfa37c34744f78fd0740ac772c816"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-6b72c017a5dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-81-f84d7d38cc1a>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(n_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Train accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_numb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_numb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-f898801a5e30>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#print (y_true)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#print (y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450030c291024b9b900d79201815a16d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train accuracy:  0.986\n"
     ]
    }
   ],
   "source": [
    "print (\"Train accuracy: \", get_accuracy(np.array(prediction(model, X_train_numb)), np.array(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"best_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(n_vocab, 100))\n",
    "model1.add(Convolution1D(128, 4, padding='same', activation='relu'))\n",
    "model1.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model1.compile('rmsprop', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.load_weights(\"best_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_one_query(query, model):\n",
    "    query = padd_data([query.split()], value=0)[0]\n",
    "    query = np.array(list(map(lambda x: word2numb[x], query)))\n",
    "    pred = model.predict_on_batch(query[np.newaxis,:])\n",
    "    pred = np.argmax(pred,-1)[0]\n",
    "    return list(map(lambda x: numb2tag[x], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"new users by source for country less germany\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = np.array(s.split())\n",
    "t = list(map(lambda x: word2numb[x], ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[176, 293, 52, 256, 115, 75, 151, 119]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-metric', 'I-metric', 'stop_word', 'B-dimension', 'stop_word', 'B-filter', 'stop_word', 'B-filter_value', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word', 'stop_word']\n",
      "CPU times: user 394 ms, sys: 227 ms, total: 622 ms\n",
      "Wall time: 606 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print (predict_one_query(s, model=model1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### С этого места фичи из CRF NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlu.cfgrammar.GADictionaryGroup import *\n",
    "from nlu.cfgrammar.GrammarBuilder import *\n",
    "from nlu.cfgrammar.DictionaryGroup import *\n",
    "from nlu.cfgrammar.DictionaryGroupsFetcher import *\n",
    "import random\n",
    "import nltk\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ga_groups = DictionaryGroupsFetcher.fetch_dictionary_groups(integration_ids=[20431])\n",
    "metric_dictionary = ga_groups[0][0].original_metric_dictionary\n",
    "dimension_dictionary = ga_groups[0][0].original_dimension_dictionary\n",
    "filter_dictionary = ga_groups[0][0].original_filter_dictionary\n",
    "predefined_filter_dictionary = ga_groups[0][0].original_predefined_filter_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    def __init__(self, dictionaries={}, stop_words={}, filter_values=[]):\n",
    "        self.dictionaries = dictionaries\n",
    "        self.stop_words = stop_words\n",
    "        self.filter_values = filter_values\n",
    "        self.dic_word_frequency = {}\n",
    "\n",
    "    def split_names_in_dictionaries(self):\n",
    "        for dictionary_name in self.dictionaries:\n",
    "            for i in range(len(self.dictionaries[dictionary_name])):\n",
    "                for j in range(len(self.dictionaries[dictionary_name][i][1])):\n",
    "                    try:\n",
    "                        self.dictionaries[dictionary_name][i][1][j] = self.dictionaries[dictionary_name][i][1][j].split()\n",
    "                    except:\n",
    "                        self.dictionaries[dictionary_name][i][1][j] = []\n",
    "\n",
    "    def number_of_uniq_words_in_dictionary(self, splitted_dictionary):\n",
    "        return len(set([token for name in splitted_dictionary for syn in name[1] for token in syn]))\n",
    "\n",
    "    def add_stop_word(self):\n",
    "        return self.stop_words['stop_word'][rndint(0, len(self.stop_words['stop_word']))]\n",
    "    \n",
    "    \n",
    "    def precalculation(self, word, splitted_dictionary):\n",
    "        return set([(ind1, ind2) for ind1, name in enumerate(splitted_dictionary) \n",
    "                                for ind2, syn in enumerate(name[1]) if word in syn])\n",
    "    \n",
    "    def get_all_words(self, splitted_dictionary):\n",
    "        return list(set([token for name in splitted_dictionary for syn in name[1] for token in syn]))\n",
    "    \n",
    "    def construct_dictionary_frequency(self, big_dictionary):\n",
    "        list_of_all_words = []\n",
    "        for name_dic in big_dictionary:\n",
    "            list_of_all_words += self.get_all_words(big_dictionary[name_dic])\n",
    "        for word in list_of_all_words:\n",
    "            self.dic_word_frequency[word] = {}\n",
    "            for name_dic in big_dictionary:\n",
    "                self.dic_word_frequency[word][name_dic] = self.precalculation(word, big_dictionary[name_dic])\n",
    "                \n",
    "    def findsubsets(self, S, m):\n",
    "        return list(set(itertools.combinations(S, m)))\n",
    "    \n",
    "    def generate_all_names(self, dictionary):\n",
    "        return[self.findsubsets(name, i)[0] for name in dictionary for i in range(1, len(name)+1)]\n",
    "            \n",
    "    def generate_rules(self, position, query, structure, filling):\n",
    "        if position == -1:\n",
    "            self.generate_rules(position + 1, query, structure, filling)\n",
    "            self.generate_rules(position + 1, query + 'show ', structure + ['sw'], filling)\n",
    "            self.generate_rules(position + 1, query + 'show me ', structure + ['sw', 'sw'], filling)\n",
    "            #self.generate_rules(position + 1, query + 'show me please ', structure + ['sw', 'sw', 'sw'], filling)\n",
    "        if position == 0:\n",
    "            self.generate_rules(position + 1, query + '{m1}', structure + ['m1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + '{m1} and {m2}', structure + ['m1', 'sw', 'm2'], filling+2)\n",
    "            #self.generate_rules(position + 1, query + '{m1} {m2}', structure + ['m1', 'm2'], filling+1)\n",
    "            self.generate_rules(position + 1, query + '{m1} vs {m2}', structure + ['m1', 'sw', 'm2'], filling+2)\n",
    "            self.generate_rules(position + 3, query + '{pf1} {m1}', structure + ['pf1', 'm1'], filling+1)\n",
    "        elif position == 1:\n",
    "            self.generate_rules(position + 1, query, structure, filling)\n",
    "            self.generate_rules(position + 1, query + ' {d1}', structure + ['d1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + ' by {d1}', structure + ['sw', 'd1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + ' for {d1}', structure + ['sw', 'd1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + ' by {d1} and {d2}', structure + ['sw', 'd1', 'sw', 'd2'], filling+2)\n",
    "            #self.generate_rules(position + 1, query + ' by {d1} and by {d2}',\n",
    "            #                    structure + ['sw', 'd1', 'sw', 'sw', 'd2'], filling+2)\n",
    "        elif position == 2:\n",
    "            if filling == 3:\n",
    "                filling = 10\n",
    "            self.generate_rules(position + 1, query, structure, filling)\n",
    "            self.generate_rules(position + 1, query + ' {pf1}', structure + ['pf1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + ' for {pf1}', structure + ['sw', 'pf1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + ' where {pf1}', structure + ['sw', 'pf1'], filling+1)\n",
    "        elif position == 3:\n",
    "            if filling == 3:\n",
    "                filling = 10\n",
    "            self.generate_rules(position + 1, query, structure, filling)\n",
    "            self.generate_rules(position + 1, query + ' {f1}', structure + ['f1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + ' for {f1}', structure + ['sw', 'f1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + ' for {f1} and {f2}', structure + ['sw', 'f1', 'sw', 'f2'], filling+2)\n",
    "            self.generate_rules(position + 1, query + ' for {f1} and for {f2}', structure + ['sw', 'f1', 'sw', 'sw', 'f2'], filling+2)\n",
    "            self.generate_rules(position + 2, query + ' {fv1} {f1}', structure + ['fv1', 'f1'], filling+2)\n",
    "        elif position == 4 and 'f1' in structure[-1]:\n",
    "            if filling == 3:\n",
    "                filling = 10\n",
    "            self.generate_rules(position + 1, query + ' contains {fv1}', structure + ['sw', 'fv1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + ' less {fv1}', structure + ['sw', 'fv1'], filling+1)\n",
    "            self.generate_rules(position + 1, query + ' greater {fv1}', structure + ['sw', 'fv1'], filling+1)\n",
    "            #self.generate_rules(position + 1, query + ' greater than {fv1}', structure + ['sw', 'sw', 'fv1'], filling+1)\n",
    "            #self.generate_rules(position + 1, query + ' equal {fv1}', structure + ['sw', 'fv1'], filling+1)\n",
    "        else:\n",
    "            self.rules.append([query, structure])\n",
    "        \n",
    "    def generate_queries_denis(self):\n",
    "        \n",
    "        '''\n",
    "        self.metrics = [token for metric in self.dictionaries['metric_dictionary'] for token in metric[1]]\n",
    "        self.dimensions = [token for dimension in self.dictionaries['dimension_dictionary'] for token in dimension[1]]\n",
    "        self.filters = [token for filt in self.dictionaries['filter_dictionary'] for token in filt[1]]\n",
    "        self.predefined_filters = [token for predefined_filter in self.dictionaries['predefined_filter_dictionary'] for\n",
    "                                   token in predefined_filter[1]]\n",
    "        '''\n",
    "        self.split_names_in_dictionaries()\n",
    "        \n",
    "        self.metrics = [token for metric in self.dictionaries['metric_dictionary'] for token in metric[1]]\n",
    "        self.dimensions = [token for dimension in self.dictionaries['dimension_dictionary'] for token in dimension[1]]\n",
    "        self.filters = [token for filt in self.dictionaries['filter_dictionary'] for token in filt[1]]\n",
    "        self.predefined_filters = [token for predefined_filter in self.dictionaries['predefined_filter_dictionary'] for\n",
    "                                   token in predefined_filter[1]]\n",
    "\n",
    "        self.construct_dictionary_frequency({'metric': self.dictionaries['metric_dictionary'],\n",
    "                                             'dimension': self.dictionaries['dimension_dictionary'],\n",
    "                                             'filter': self.dictionaries['filter_dictionary'],\n",
    "                                             'predefined_filter': self.dictionaries['predefined_filter_dictionary']})\n",
    "        \n",
    "        self.total_numbers = np.array(\n",
    "            [self.number_of_uniq_words_in_dictionary(self.dictionaries[d_name]) for d_name in self.dictionaries])\n",
    "\n",
    "        self.rules = []\n",
    "        self.generate_rules(-1, \"\", [], 0)\n",
    "\n",
    "        self.queries, self.queries_with_tags, self.labels = [], [], []\n",
    "        \n",
    "        '''\n",
    "        print(self.metrics)\n",
    "        self.metrics = [['page', 'view', 'per', 'session'], ['avg', 'session', 'duration'], ['new', 'user'], ['conversions', 'rate'], ['signup'], ['users'], ['signup']]\n",
    "        self.dimensions = [['social', 'activity', 'network', 'action'], ['query', 'product', 'name'], ['social', 'type'], ['browser', 'version'], ['product', 'name'], ['source']]\n",
    "        self.predefined_filters = [['tablet', 'and', 'desktop', 'traffic'], ['mobile', 'and', 'tablet'], ['new', 'users'], ['ios'], ['tablet', 'traffic']]\n",
    "        self.filters = [['social', 'activity', 'network', 'action'], ['internal', 'promotion', 'name'], ['device', 'category'], ['app'], ['user', 'gender'], ['network', 'location'], ['app', 'name'] ]\n",
    "        print(self.metrics)\n",
    "        '''\n",
    "        self.metrics_list = self.generate_all_names(self.metrics)\n",
    "        self.dimensions_list = self.generate_all_names(self.dimensions)\n",
    "        self.predefined_filters_list = self.generate_all_names(self.predefined_filters)\n",
    "        self.filters_list = self.generate_all_names(self.filters)\n",
    "\n",
    "        self.cartmetric1 = cartesian([self.metrics_list, self.metrics_list])\n",
    "        self.cartdimension1 = cartesian([self.dimensions_list, self.dimensions_list])\n",
    "        self.cartfilter1 = cartesian([self.filters_list, self.filters_list])\n",
    "        self.cartpredfilter1 = cartesian([self.predefined_filters_list, self.predefined_filters_list])\n",
    "                        \n",
    "        ind = -1\n",
    "        for rule in tqdm(self.rules):\n",
    "            ind += 1\n",
    "            numbers1 = [('m1' in rule[1]) * len(self.metrics_list) + ('m1' not in rule[1]),\n",
    "                        ('d1' in rule[1]) * len(self.dimensions_list) + ('d1' not in rule[1]),\n",
    "                        ('f1' in rule[1]) * len(self.filters_list) + ('f1' not in rule[1]),\n",
    "                        ('pf1' in rule[1]) * len(self.predefined_filters_list) + ('pf1' not in rule[1]),\n",
    "                        ('fv1' in rule[1]) * 2 + ('fv1' not in rule[1])]\n",
    "            \n",
    "            if ind % 4 == 0:\n",
    "                np.random.shuffle(self.cartmetric1)\n",
    "                np.random.shuffle(self.cartdimension1)\n",
    "                np.random.shuffle(self.cartfilter1)\n",
    "                np.random.shuffle(self.cartpredfilter1)\n",
    "            \n",
    "            if 'm2' in rule[1]:\n",
    "                numbers1[0] *= len(self.metrics_list)\n",
    "            if 'd2' in rule[1]:\n",
    "                numbers1[1] *= len(self.dimensions_list)\n",
    "            if 'f2' in rule[1]:\n",
    "                numbers1[2] *= len(self.filters_list)\n",
    "            if 'pf2' in rule[1]:\n",
    "                numbers1[3] *= len(self.predefined_filters_list)\n",
    "                \n",
    "            \n",
    "            self.cartmetric = self.cartmetric1[:2]\n",
    "            self.cartdimension = self.cartdimension1[:2]\n",
    "            self.cartfilter = self.cartfilter1[:2]\n",
    "            self.cartpredfilter = self.cartpredfilter1[:2]\n",
    "            \n",
    "            filter_values = np.array(self.filter_values)[rndint(0, len(self.filter_values),\n",
    "                                                                numbers1[4])]\n",
    "            for met1, met2 in self.cartmetric:\n",
    "                for dim1, dim2 in self.cartdimension:\n",
    "                    for fil1, fil2 in self.cartfilter:\n",
    "                        for pfil1, pfil2 in self.cartpredfilter:\n",
    "                            for filv1 in self.filter_values:\n",
    "                                label = []\n",
    "                                for tag in rule[1]:\n",
    "                                    if tag == 'm1':\n",
    "                                        for i in range(len(met1)):\n",
    "                                            if i == 0:\n",
    "                                                label.append('B-metric')\n",
    "                                            else:\n",
    "                                                label.append('I-metric')\n",
    "\n",
    "                                    if tag == 'm2':\n",
    "                                        for i in range(len(met2)):\n",
    "                                            if i == 0:\n",
    "                                                label.append('B-metric')\n",
    "                                            else:\n",
    "                                                label.append('I-metric')\n",
    "\n",
    "                                    if tag == 'd1':\n",
    "                                        for i in range(len(dim1)):\n",
    "                                            if i == 0:\n",
    "                                                label.append('B-dimension')\n",
    "                                            else:\n",
    "                                                label.append('I-dimension')\n",
    "\n",
    "                                    if tag == 'd2':\n",
    "                                        for i in range(len(dim2)):\n",
    "                                            if i == 0:\n",
    "                                                label.append('B-dimension')\n",
    "                                            else:\n",
    "                                                label.append('I-dimension')\n",
    "\n",
    "                                    if tag == 'f1':\n",
    "                                        for i in range(len(fil1)):\n",
    "                                            if i == 0:\n",
    "                                                label.append('B-filter')\n",
    "                                            else:\n",
    "                                                label.append('I-filter')\n",
    "\n",
    "                                    if tag == 'f2':\n",
    "                                        for i in range(len(fil2)):\n",
    "                                            if i == 0:\n",
    "                                                label.append('B-filter')\n",
    "                                            else:\n",
    "                                                label.append('I-filter')\n",
    "\n",
    "                                    if tag == 'pf1':\n",
    "                                        for i in range(len(pfil1)):\n",
    "                                            if i == 0:\n",
    "                                                label.append('B-predefined_filter')\n",
    "                                            else:\n",
    "                                                label.append('I-predefined_filter')\n",
    "\n",
    "                                    if tag == 'pf2':\n",
    "                                        for i in range(len(pfil2)):\n",
    "                                            if i == 0:\n",
    "                                                label.append('B-predefined_filter')\n",
    "                                            else:\n",
    "                                                label.append('I-predefined_filter')\n",
    "                                                \n",
    "                                    if tag == 'sw':\n",
    "                                        label.append('stop_word')\n",
    "\n",
    "                                    if tag == 'fv1':\n",
    "                                        for i in range(len(filv1.split())):\n",
    "                                            if i == 0:\n",
    "                                                label.append('B-filter_value')\n",
    "                                            else:\n",
    "                                                label.append('I-filter_value')\n",
    "                                self.queries_with_tags.append(tuple(list(zip(rule[0].format(d1=' '.join(list(dim1)), \n",
    "                                                                                      m1=' '.join(list(met1)),\n",
    "                                                                                      f1=' '.join(list(fil1)),\n",
    "                                                                                      pf1=' '.join(list(pfil1)),\n",
    "                                                                                      d2=' '.join(list(dim2)),\n",
    "                                                                                      m2=' '.join(list(met2)),\n",
    "                                                                                      f2=' '.join(list(fil2)),\n",
    "                                                                                      pf2=' '.join(list(pfil2)),\n",
    "                                                                                      fv1=filv1).split(),\n",
    "                                                                                      label))))\n",
    "        \n",
    "                   \n",
    "    def construct_name(self, word):\n",
    "        return [word.split()[i] for i in sorted(list(set(rndint(0, len(word.split()), rndint(0, len(word.split()), 1)))))]\n",
    "    \n",
    "    def sent2features(self, sent):\n",
    "        return [self.word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "    def sent2labels(self, sent):\n",
    "        return [label for token, label in sent]\n",
    "\n",
    "    def sent2tokens(self, sent):\n",
    "        return [token for token, label in sent]  \n",
    "    \n",
    "    def add_features_to_words(self, words):\n",
    "        res = np.zeros(4)\n",
    "        names = ['metric', 'dimension', 'filter', 'predefined_filter']\n",
    "        for ind, name in enumerate(names):\n",
    "            try:\n",
    "                c = self.dic_word_frequency[words[0]][name]\n",
    "            except: \n",
    "                c = set()\n",
    "            for word in words[1:]:\n",
    "                try:\n",
    "                    c = c.intersection(self.dic_word_frequency[word][name])\n",
    "                except:\n",
    "                    c = set()\n",
    "            res[ind] = len(c)\n",
    "        ans = np.argmax(res)\n",
    "        res = np.zeros(4)\n",
    "        res[ans] = 1\n",
    "        return res\n",
    "\n",
    "    def add_features_ratio(self, words):\n",
    "        res = np.zeros(4)\n",
    "        names = ['metric', 'dimension', 'filter', 'predefined_filter']\n",
    "        for ind, name in enumerate(names):\n",
    "            try:\n",
    "                c = self.dic_word_frequency[words[0]][name]\n",
    "            except: \n",
    "                c = set()\n",
    "            for word in words[1:]:\n",
    "                try:\n",
    "                    c = c.intersection(self.dic_word_frequency[word][name])\n",
    "                except:\n",
    "                    c = set()\n",
    "            if len(c) == 0:\n",
    "                res[ind] = 0\n",
    "            else:\n",
    "                res[ind] = max([len(words) / len(self.dictionaries[name+\"_dictionary\"][pos[0]][1][pos[1]]) for pos in c])\n",
    "        return res\n",
    "\n",
    "    def word2features(self, sent, i):\n",
    "        word = str(sent[i][0])\n",
    "        # dict_features_non_stop = self.add_dictionary_features_to_word_non_stop(word)\n",
    "        dict_features0 = self.add_features_to_words([word])\n",
    "        features = list(itertools.chain.from_iterable([\n",
    "            ['word.isdigit=%s' % word.isdigit(),\n",
    "            'from_start=' + str(i),\n",
    "            'from_end=' + str(len(sent) - i),\n",
    "            'prop_start=' + str(i / len(sent)),\n",
    "            'sent_len=' + str(len(sent)),\n",
    "            'pos_tag=' + nltk.pos_tag([word])[0][1],],\n",
    "            \n",
    "            self.add_features_words_tag([word], '0:'),\n",
    "            self.add_important_words_features_tag(word, '0:'),\n",
    "            \n",
    "        ]))\n",
    "\n",
    "        if i > 0:\n",
    "            word1 = str(sent[i - 1][0])\n",
    "            dict_features = self.add_features_to_words([word1])\n",
    "            dict_features1 = self.add_features_to_words([word, word1])\n",
    "            features.extend(list(itertools.chain.from_iterable([\n",
    "                ['-1:word.isdigit=%s' % word1.isdigit(),\n",
    "                '-1:pos_tag=' + nltk.pos_tag([word1])[0][1],],\n",
    "                \n",
    "                self.add_features_words_tag([word], '-1:'),\n",
    "                self.add_features_words_tag([word, word1], '-1.1:'),\n",
    "                self.add_important_words_features_tag(word1, '-1:'),\n",
    "                \n",
    "                \n",
    "            ])))\n",
    "        else:\n",
    "            features.append('BOS')\n",
    "\n",
    "        if i > 1 and i < len(sent):\n",
    "            word1 = str(sent[i - 2][0])\n",
    "            dict_features1 = self.add_features_to_words([word, word1])\n",
    "            features.extend(list(itertools.chain.from_iterable([\n",
    "                ['-2:word.isdigit=%s' % word1.isdigit(),\n",
    "                '-2:pos_tag=' + nltk.pos_tag([word1])[0][1],],\n",
    "                \n",
    "                self.add_features_words_tag([word, word1], '-2:'),\n",
    "                self.add_important_words_features_tag(word1, '-2:'),\n",
    "                \n",
    "            ])))\n",
    "        else:\n",
    "            features.append('BOS')\n",
    "\n",
    "        if i < len(sent) - 1:\n",
    "            word1 = str(sent[i + 1][0])\n",
    "            dict_features = self.add_features_to_words([word1])\n",
    "            dict_features1 = self.add_features_to_words([word, word1])\n",
    "            features.extend(list(itertools.chain.from_iterable([\n",
    "\n",
    "                ['+1:word.isdigit=%s' % word1.isdigit(),\n",
    "                '+1:pos_tag=' + nltk.pos_tag([word1])[0][1],],\n",
    "                \n",
    "                self.add_features_words_tag([word], '+1:'),\n",
    "                self.add_features_words_tag([word, word1], '+1.1:'),\n",
    "                self.add_important_words_features_tag(word1, '+1:'),\n",
    "                \n",
    "            ])))\n",
    "        else:\n",
    "            features.append('BOS')\n",
    "\n",
    "        if i < len(sent) - 2:\n",
    "            word1 = str(sent[i + 2][0])\n",
    "            dict_features = self.add_features_to_words([word1])\n",
    "            dict_features1 = self.add_features_to_words([word, word1])\n",
    "            features.extend(list(itertools.chain.from_iterable([\n",
    "\n",
    "                ['+2:word.isdigit=%s' % word1.isdigit(),\n",
    "                '+2:pos_tag=' + nltk.pos_tag([word1])[0][1],],\n",
    "                \n",
    "                self.add_features_words_tag([word, word1], '+2:'),\n",
    "                self.add_important_words_features_tag(word1, '+2:'),\n",
    "                \n",
    "                \n",
    "            ])))\n",
    "        else:\n",
    "            features.append('BOS')\n",
    "\n",
    "        if i > 0 and i < len(sent) - 1:\n",
    "            word = str(sent[i][0])\n",
    "            word1 = str(sent[i - 1][0])\n",
    "            word2 = str(sent[i + 1][0])\n",
    "            dict_features = self.add_features_to_words([word, word1, word2])\n",
    "            #ratio_features = self.add_features_ratio([word, word1, word2])\n",
    "            features.extend(list(itertools.chain.from_iterable([\n",
    "                ['+1-1.0=' + str(dict_features[0]),\n",
    "                '+1-1.1=' + str(dict_features[1]),\n",
    "                '+1-1.2=' + str(dict_features[2]),\n",
    "                '+1-1.3=' + str(dict_features[3]),\n",
    "                ],\n",
    "            ])))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def sent2features(self, sent):\n",
    "        return [self.word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "    def sent2labels(self, sent):\n",
    "        return [label for token, label in sent]\n",
    "\n",
    "    def sent2tokens(self, sent):\n",
    "        return [token for token, label in sent]\n",
    "\n",
    "    def train_test(self, split=0.5):\n",
    "        self.X_train = [self.sent2features(s) for s in\n",
    "                        tqdm(self.queries_with_tags[0:int(len(self.queries_with_tags) * split)])]\n",
    "        self.y_train = [self.sent2labels(s) for s in self.queries_with_tags[0:int(len(self.queries_with_tags) * split)]]\n",
    "        self.X_test = [self.sent2features(s) for s in\n",
    "                       tqdm(self.queries_with_tags[int(len(self.queries_with_tags) * split) + 1:])]\n",
    "        self.y_test = [self.sent2labels(s) for s in\n",
    "                       self.queries_with_tags[int(len(self.queries_with_tags) * split) + 1:]]\n",
    "\n",
    "    def add_features_to_query(self, query):\n",
    "        \n",
    "        q = [[token] for token in query.split()]\n",
    "        self.total_numbers = np.array(\n",
    "            [self.number_of_uniq_words_in_dictionary(self.dictionaries[d_name]) for d_name in self.dictionaries])\n",
    "        return [self.sent2features(q)]\n",
    "    \n",
    "    def add_custom_query(self, words, tokens):\n",
    "        DG.queries_with_tags.append(list(zip(words, tokens)))\n",
    "        \n",
    "    def add_important_words_features_tag(self, word, tag='0:'):\n",
    "        list_of_words = ['by', 'for', 'where', 'with', 'contains', 'greater', 'less', 'equal']\n",
    "        return [tag+w+'='+str(word==w) for w in list_of_words]\n",
    "    \n",
    "    def add_features_words_tag(self, words, tag='0:'):\n",
    "        return [tag+str(ind)+'='+str(f) for ind, f in enumerate(self.add_features_to_words(words))]\n",
    "    \n",
    "    def add_features_ratio_tag(self, words, tag='0:'):\n",
    "        return [tag+str(ind)+'='+str(f) for ind, f in enumerate(self.add_features_ratio(words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionaries = {'metric_dictionary': metric_dictionary,\n",
    "                  'dimension_dictionary': dimension_dictionary,\n",
    "                  'filter_dictionary': filter_dictionary, \n",
    "                  'predefined_filter_dictionary': predefined_filter_dictionary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DG = DatasetGenerator(dictionaries=dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DG.split_names_in_dictionaries()\n",
    "DG.construct_dictionary_frequency({'metric': DG.dictionaries['metric_dictionary'],\n",
    "                                   'dimension': DG.dictionaries['dimension_dictionary'],\n",
    "                                   'filter': DG.dictionaries['filter_dictionary'],\n",
    "                                   'predefined_filter': DG.dictionaries['predefined_filter_dictionary']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"new users by source in germany\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = DG.add_features_to_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55, 78, 92, 92, 78, 55]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
